
@article{manber_suffix_1993,
	title = {Suffix {Arrays}: {A} {New} {Method} for {On}-{Line} {String} {Searches}},
	volume = {22},
	issn = {0097-5397},
	shorttitle = {Suffix {Arrays}},
	url = {https://epubs.siam.org/doi/10.1137/0222058},
	doi = {10.1137/0222058},
	abstract = {A new and conceptually simple data structure, called a suffix array, for on-line string searches is introduced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, “Is W a substring of A?” to be answered in time \$O(P + {\textbackslash}log N)\$, where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in \$O(N)\$ time in the worst case, versus \$O(N{\textbackslash}log N)\$ time for suffix arrays. However, an augmented algorithm is given that, regardless of the alphabet size, constructs suffix arrays in \$O(N)\$expected time, albeit with lesser space efficiency. It is believed that suffix arrays will prove to be better in practice than suffix trees for many applications.},
	number = {5},
	urldate = {2020-12-08},
	journal = {SIAM Journal on Computing},
	author = {Manber, Udi and Myers, Gene},
	month = oct,
	year = {1993},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {935--948},
	file = {Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\4IGVDXKR\\0222058.html:text/html},
}

@article{nevill-manning_identifying_1997,
	title = {Identifying {Hierarchical} {Structure} in {Sequences}: {A} linear-time algorithm},
	shorttitle = {Identifying {Hierarchical} {Structure} in {Sequences}},
	url = {http://arxiv.org/abs/cs/9709102},
	abstract = {SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.},
	urldate = {2020-12-07},
	journal = {arXiv:cs/9709102},
	author = {Nevill-Manning, C. G. and Witten, I. H.},
	month = aug,
	year = {1997},
	note = {arXiv: cs/9709102},
	keywords = {Computer Science - Artificial Intelligence, \_read},
	annote = {Comment: See http://www.jair.org/ for an online appendix and other files accompanying this article},
	file = {arXiv.org Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\FVY2HI4C\\9709102.html:text/html;arXiv Fulltext PDF:C\:\\Users\\etiet\\Zotero\\storage\\7V9IUA7V\\Nevill-Manning und Witten - 1997 - Identifying Hierarchical Structure in Sequences A.pdf:application/pdf},
}

@inproceedings{larsson_offline_1999,
	address = {Snowbird, UT, USA},
	title = {Offline dictionary-based compression},
	isbn = {978-0-7695-0096-6},
	url = {http://ieeexplore.ieee.org/document/755679/},
	doi = {10.1109/DCC.1999.755679},
	urldate = {2020-12-07},
	booktitle = {Proceedings {DCC}'99 {Data} {Compression} {Conference} ({Cat}. {No}. {PR00096})},
	publisher = {IEEE},
	author = {Larsson, N.J. and Moffat, A.},
	year = {1999},
	pages = {296--305},
	file = {Eingereichte Version:C\:\\Users\\etiet\\Zotero\\storage\\T5T3TRGB\\Larsson und Moffat - 1999 - Offline dictionary-based compression.pdf:application/pdf},
}

@article{ziv_universal_1977,
	title = {A universal algorithm for sequential data compression},
	volume = {23},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1055714/},
	doi = {10.1109/TIT.1977.1055714},
	
	number = {3},
	urldate = {2020-12-07},
	journal = {IEEE Transactions on Information Theory},
	author = {Ziv, J. and Lempel, A.},
	month = may,
	year = {1977},
	pages = {337--343},
	file = {Eingereichte Version:C\:\\Users\\etiet\\Zotero\\storage\\99KG6KHH\\Ziv und Lempel - 1977 - A universal algorithm for sequential data compress.pdf:application/pdf},
}

@article{huffman_method_1952,
	title = {A {Method} for the {Construction} of {Minimum}-{Redundancy} {Codes}},
	volume = {40},
	issn = {0096-8390},
	url = {http://ieeexplore.ieee.org/document/4051119/},
	doi = {10.1109/JRPROC.1952.273898},
	number = {9},
	urldate = {2020-12-07},
	journal = {Proceedings of the IRE},
	author = {Huffman, David},
	month = sep,
	year = {1952},
	pages = {1098--1101},
}

@inproceedings{tabei_succinct_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Succinct} {Grammar} {Compression}},
	isbn = {978-3-642-38905-4},
	doi = {10.1007/978-3-642-38905-4_23},
	abstract = {We solve an open problem related to an optimal encoding of a straight line program (SLP), a canonical form of grammar compression deriving a single string deterministically. We show that an information-theoretic lower bound for representing an SLP with n symbols requires at least 2n + logn! + o(n) bits. We then present a succinct representation of an SLP; this representation is asymptotically equivalent to the lower bound. The space is at most 2nlogρ(1 + o(1)) bits for ρ≤2n−−√ρ≤2n{\textbackslash}rho {\textbackslash}leq 2{\textbackslash}sqrt\{n\}, while supporting random access to any production rule of an SLP in O(loglogn) time. In addition, we present a novel dynamic data structure associating a digram with a unique symbol. Such a data structure is called a naming function and has been implemented using a hash table that has a space-time tradeoff. Thus, the memory space is mainly occupied by the hash table during the development of production rules. Alternatively, we build a dynamic data structure for the naming function by leveraging the idea behind the wavelet tree. The space is strictly bounded by 2nlogn(1 + o(1)) bits, while supporting O(logn) query and update time.},
	
	booktitle = {Combinatorial {Pattern} {Matching}},
	publisher = {Springer},
	author = {Tabei, Yasuo and Takabatake, Yoshimasa and Sakamoto, Hiroshi},
	editor = {Fischer, Johannes and Sanders, Peter},
	year = {2013},
	keywords = {Directed Acyclic Graph, Entropy Bound, Naming Function, Production Rule, Query Time},
	pages = {235--246},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\TYEITDX3\\Tabei et al. - 2013 - A Succinct Grammar Compression.pdf:application/pdf},
}

@article{apostolico_off-line_2000,
	title = {Off-line compression by greedy textual substitution},
	volume = {88},
	issn = {1558-2256},
	doi = {10.1109/5.892709},
	abstract = {Greedy off-line textual substitution refers to the following approach to compression or structural inference. Given a long text string x, a substring W is identified such that replacing all instances of W in X except one by a suitable pair of pointers yields the highest possible contraction of X; the process is then repeated on the contracted text string until substrings capable of producing contractions can no longer be found. This paper examines computational issues arising in the implementation of this paradigm and describes some applications and experiments.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Apostolico, A. and Lonardi, S.},
	month = nov,
	year = {2000},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Biological information theory, Biology computing, CD-ROMs, computational complexity, Computational efficiency, computational issues, contraction, Councils, data compression, Data compression, Dictionaries, Encoding, grammars, greedy textual substitution, inference mechanisms, off-line compression, Production, Statistics, structural inference, substring, text analysis, textstring, \_read},
	pages = {1733--1744},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\3SA4KGX5\\892709.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\XYSHXCXE\\Apostolico und Lonardi - 2000 - Off-line compression by greedy textual substitutio.pdf:application/pdf},
}

@inproceedings{carrascosa_choosing_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Choosing {Word} {Occurrences} for the {Smallest} {Grammar} {Problem}},
	isbn = {978-3-642-13089-2},
	doi = {10.1007/978-3-642-13089-2_13},
	abstract = {The smallest grammar problem - namely, finding a smallest context-free grammar that generates exactly one sequence - is of practical and theoretical importance in fields such as Kolmogorov complexity, data compression and pattern discovery. We propose to focus on the choice of the occurrences to be rewritten by non-terminals. We extend classical offline algorithms by introducing a global optimization of this choice at each step of the algorithm. This approach allows us to define the search space of a smallest grammar by separating the choice of the non-terminals and the choice of their occurrences. We propose a second algorithm that performs a broader exploration by allowing the removal of useless words that were chosen previously. Experiments on a classical benchmark show that our algorithms consistently find smaller grammars then state-of-the-art algorithms.},
	
	booktitle = {Language and {Automata} {Theory} and {Applications}},
	publisher = {Springer},
	author = {Carrascosa, Rafael and Coste, François and Gallé, Matthias and Infante-Lopez, Gabriel},
	editor = {Dediu, Adrian-Horia and Fernau, Henning and Martín-Vide, Carlos},
	year = {2010},
	keywords = {Production Rule, Data Compression, Kolmogorov Complexity, Pattern Discovery, Repeated Word},
	pages = {154--165},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\AHG2V2VD\\Carrascosa et al. - 2010 - Choosing Word Occurrences for the Smallest Grammar.pdf:application/pdf},
}

@book{charikar_smallest_2005,
	title = {The {Smallest} {Grammar} {Problem}},
	abstract = {This paper addresses the smallest grammar problem: What is the smallest context-free grammar that generates exactly one given string σ? This is a natural question about a fundamental object connected to many fields, including data compression, Kolmogorov complexity, pattern identification, and addition chains. Due to the problem’s inherent complexity, our objective is to find an approximation algorithm which finds a small grammar for the input string. We focus attention on the approximation ratio of the algorithm (and implicitly, worst-case behavior) to establish provable performance guarantees and to address short-comings in the classical measure of redundancy in the literature. Our first results are a variety of hardness results, most notably that every efficient algorithm for the smallest grammar problem has approximation ratio at least 8569 unless P = NP. 8568 We then bound approximation ratios for several of the bestknown grammar-based compression algorithms, including LZ78, BISECTION, SEQUENTIAL, LONGEST MATCH, GREEDY, and RE-PAIR. Among these, the best upper bound we show is O(n 1/2). We finish by presenting two novel algorithms with exponentially better ratios of O(log 3 n) and O(log(n/m ∗)), where m ∗ is the size of the smallest grammar for that input. The latter highlights a connection between grammar-based compression and LZ77.},
	author = {Charikar, Moses and Lehman, Eric and Lehman, April and Liu, Ding and Panigrahy, Rina and Prabhakaran, Manoj and Sahai, Amit and Shelat, Abhi},
	year = {2005},
	file = {Citeseer - Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\LJ539ZW4\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\MI22SQWH\\Charikar et al. - 2005 - The Smallest Grammar Problem.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\MDLKTMCS\\summary.html:text/html},
}

@article{nevill-manning_-line_2000,
	title = {On-line and off-line heuristics for inferring hierarchies of repetitions in sequences},
	volume = {88},
	issn = {1558-2256},
	doi = {10.1109/5.892710},
	abstract = {Hierarchical dictionary-based compression schemes form a grammar for a text by replacing each repeated string with a production rule. While such schemes usually operate on-line, making a replacement as soon as repetition is detected, off-line operation permits greater freedom in choosing the order of replacement. In this paper, we compare the on-line method with three off-line heuristics for selecting the next substring to replace: longest string first, most common string first, and the string that minimizes the size of the grammar locally. Surprisingly, two of the off-line techniques, like the on-line method, run in time linear in the size of the input. We evaluate each technique on artificial and natural sequences. In general, the locally-most-compressive heuristic performs best, followed by most frequent, the on-line technique, and, lagging by some distance, the longest-first technique.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Nevill-Manning, C. G. and Witten, I. H.},
	month = nov,
	year = {2000},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {data compression, Dictionaries, grammars, artificial sequences, Compression algorithms, Computer science, dictionary-based compression schemes, Frequency, grammar, Hardware, hierarchies of repetitions, knowledge based systems, locally-most-compressive heuristic, natural sequences, off-line heuristics, on-line heuristics, Probability distribution, production rule, sequences},
	pages = {1745--1755},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\WCNTRTYH\\892710.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\52EZN7JU\\Nevill-Manning und Witten - 2000 - On-line and off-line heuristics for inferring hier.pdf:application/pdf},
}

@inproceedings{benz_effective_2013,
	address = {New York, NY, USA},
	series = {{GECCO} '13},
	title = {An effective heuristic for the smallest grammar problem},
	isbn = {978-1-4503-1963-8},
	url = {https://doi.org/10.1145/2463372.2463441},
	doi = {10.1145/2463372.2463441},
	abstract = {The smallest grammar problem is the problem of finding the smallest context-free grammar that generates exactly one given sequence. Approximating the problem with a ratio of less than 8569/8568 is known to be NP-hard. Most work on this problem has focused on finding decent solutions fast (mostly in linear time), rather than on good heuristic algorithms. Inspired by a new perspective on the problem presented by Carrascosa et al.{\textbackslash} (2010), we investigate the performance of different heuristics on the problem. The aim is to find a good solution on large instances by allowing more than linear time. We propose a hybrid of a max-min ant system and a genetic algorithm that in combination with a novel local search outperforms the state of the art on all files of the Canterbury corpus, a standard benchmark suite. Furthermore, this hybrid performs well on a standard DNA corpus.},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 15th annual conference on {Genetic} and evolutionary computation},
	publisher = {Association for Computing Machinery},
	author = {Benz, Florian and Kötzing, Timo},
	month = jul,
	year = {2013},
	keywords = {ant colony optimization, evolutionary computation, smallest grammar problem},
	pages = {487--494},
	file = {Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\FCI8ESJ2\\Benz und Kötzing - 2013 - An effective heuristic for the smallest grammar pr.pdf:application/pdf},
}

@inproceedings{fischer_inducing_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Inducing the {LCP}-{Array}},
	isbn = {978-3-642-22300-6},
	doi = {10.1007/978-3-642-22300-6_32},
	abstract = {We show how to modify the linear-time construction algorithm for suffix arrays based on induced sorting (Nong et al., DCC’09) such that it computes the array of longest common prefixes (LCP-array) as well. Practical tests show that this outperforms recent LCP-array construction algorithms (Gog and Ohlebusch, ALENEX’11).},
	
	booktitle = {Algorithms and {Data} {Structures}},
	publisher = {Springer},
	author = {Fischer, Johannes},
	editor = {Dehne, Frank and Iacono, John and Sack, Jörg-Rüdiger},
	year = {2011},
	keywords = {Array Construction, Current Head, Linear Time, Range Minimum, Suffix Array},
	pages = {374--385},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\3YQBQ869\\Fischer - 2011 - Inducing the LCP-Array.pdf:application/pdf},
}

@inproceedings{kasai_linear-time_2001,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Linear-{Time} {Longest}-{Common}-{Prefix} {Computation} in {Suffix} {Arrays} and {Its} {Applications}},
	isbn = {978-3-540-48194-2},
	doi = {10.1007/3-540-48194-X_17},
	abstract = {We present a linear-time algorithm to compute the longest common prefix information in suffix arrays. As two applications of our algorithm, we show that our algorithm is crucial to the effective use of block-sorting compression, and we present a linear-time algorithm to sim- ulate the bottom-up traversal of a suffix tree with a suffix array combined with the longest common prefix information.},
	
	booktitle = {Combinatorial {Pattern} {Matching}},
	publisher = {Springer},
	author = {Kasai, Toru and Lee, Gunho and Arimura, Hiroki and Arikawa, Setsuo and Park, Kunsoo},
	editor = {Landau, Gad M. and Amir, Amihood},
	year = {2001},
	keywords = {Suffix Array, Compact Tree, Internal Node, Lower Common Ancestor, Text Database},
	pages = {181--192},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\A97JL2AW\\Kasai et al. - 2001 - Linear-Time Longest-Common-Prefix Computation in S.pdf:application/pdf},
}

@techreport{burrows_block-sorting_1994,
	title = {A block-sorting lossless data compression algorithm},
	author = {Burrows, M. and Wheeler, D. J.},
	year = {1994},
	file = {Citeseer - Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\22NA9X4T\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\UTKN8MUV\\Burrows und Wheeler - 1994 - A block-sorting lossless data compression algorith.pdf:application/pdf},
}

@article{nong_two_2011,
	title = {Two {Efficient} {Algorithms} for {Linear} {Time} {Suffix} {Array} {Construction}},
	volume = {60},
	issn = {1557-9956},
	doi = {10.1109/TC.2010.188},
	abstract = {We present, in this paper, two efficient algorithms for linear time suffix array construction. These two algorithms achieve their linear time complexities, using the techniques of divide-and-conquer, and recursion. What distinguish the proposed algorithms from other linear time suffix array construction algorithms (SACAs) are the variable-length leftmost S-type (LMS) substrings and the fixed-length d-critical substrings sampled for problem reduction, and the simple algorithms for sorting these sampled substrings: the induced sorting algorithm for the variable-length LMS substrings and the radix sorting algorithm for the fixed-length d-critical substrings. The very simple sorting mechanisms render our algorithms an elegant design framework, and, in turn, the surprisingly succinct implementations. The fully functional sample implementations of our proposed algorithms require only around 100 lines of C code for each, which is only 1/10 of the implementation of the KA algorithm and comparable to that of the KS algorithm. The experimental results demonstrate that these two newly proposed algorithms yield the best time and space efficiencies among all the existing linear time SACAs.},
	number = {10},
	journal = {IEEE Transactions on Computers},
	author = {Nong, G. and Zhang, S. and Chan, W. H.},
	month = oct,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {computational complexity, Algorithm design and analysis, Arrays, Construction industry, data structures, design framework, divide-and-conquer technique, divide-and-conquer., fixed-length d-critical substrings, Indexes, induced sorting algorithm, Least squares approximation, linear time, linear time complexity, linear time suffix array construction, Merging, radix sorting algorithm, recursion technique, sampled substrings sorting, sorting, Sorting, Suffix array, variable-length leftmost S-type substrings},
	pages = {1471--1484},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\7MYQRNVY\\5582081.html:text/html},
}

@article{abouelhoda_replacing_2004,
	series = {The 9th {International} {Symposium} on {String} {Processing} and {Information} {Retrieval}},
	title = {Replacing suffix trees with enhanced suffix arrays},
	volume = {2},
	issn = {1570-8667},
	url = {http://www.sciencedirect.com/science/article/pii/S1570866703000650},
	doi = {10.1016/S1570-8667(03)00065-0},
	abstract = {The suffix tree is one of the most important data structures in string processing and comparative genomics. However, the space consumption of the suffix tree is a bottleneck in large scale applications such as genome analysis. In this article, we will overcome this obstacle. We will show how every algorithm that uses a suffix tree as data structure can systematically be replaced with an algorithm that uses an enhanced suffix array and solves the same problem in the same time complexity. The generic name enhanced suffix array stands for data structures consisting of the suffix array and additional tables. Our new algorithms are not only more space efficient than previous ones, but they are also faster and easier to implement.},
	
	number = {1},
	urldate = {2020-12-10},
	journal = {Journal of Discrete Algorithms},
	author = {Abouelhoda, Mohamed Ibrahim and Kurtz, Stefan and Ohlebusch, Enno},
	month = mar,
	year = {2004},
	keywords = {Suffix array, Genome comparison, Pattern matching, Repeat analysis, Suffix tree},
	pages = {53--86},
	file = {ScienceDirect Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\JXK2GBBN\\S1570866703000650.html:text/html;ScienceDirect Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\P5PYWBRC\\Abouelhoda et al. - 2004 - Replacing suffix trees with enhanced suffix arrays.pdf:application/pdf},
}

@article{kieffer_grammar-based_2000,
	title = {Grammar-based codes: a new class of universal lossless source codes},
	volume = {46},
	issn = {1557-9654},
	shorttitle = {Grammar-based codes},
	doi = {10.1109/18.841160},
	abstract = {We investigate a type of lossless source code called a grammar-based code, which, in response to any input data string x over a fixed finite alphabet, selects a context-free grammar G/sub x/ representing x in the sense that x is the unique string belonging to the language generated by G/sub x/. Lossless compression of x takes place indirectly via compression of the production rules of the grammar G/sub x/. It is shown that, subject to some mild restrictions, a grammar-based code is a universal code with respect to the family of finite-state information sources over the finite alphabet. Redundancy bounds for grammar-based codes are established. Reduction rules for designing grammar-based codes are presented.},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Kieffer, J. C. and {En-Hui Yang}},
	month = may,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {data compression, context-free grammar, context-free grammars, finite-state information sources, grammar-based codes, input data string, lossless compression, production rules, reduction rules, redundancy, Redundancy, redundancy bounds, source coding, universal lossless source codes},
	pages = {737--754},
	file = {Eingereichte Version:C\:\\Users\\etiet\\Zotero\\storage\\757ZYZQ7\\Kieffer und En-Hui Yang - 2000 - Grammar-based codes a new class of universal loss.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\B29REYG8\\841160.html:text/html},
}

@misc{mahdi_implementing_2012,
	title = {Implementing a {Novel} {Approach} an {Convert} {Audio} {Compression} to {Text} {Coding} via {Hybrid} {Technique}},
	url = {/paper/Implementing-a-Novel-Approach-an-Convert-Audio-to-Mahdi-Mohammed/f31713a7d75d6a188d3ada5e89f9243d699e61d4},
	abstract = {Compression is the reduction in size of data in order to save space or transmission time. For data transmission, compression can be performed on just the data content or on the entire transmission unit (including header data) depending on a number of factors. In this study, we considered the application of an audio compression method by using text coding where audio compression represented via convert audio file to text file for reducing the time to data transfer by communication channel. Approach: we proposed two coding methods are applied to optimizing the solution by using CFG. Results: we test our application by using 4-bit coding algorithm the results of this method show not satisfy then we proposed a new approach to compress audio files, by converting an audio file in to a text file and then compressing the new text file by using the common compression techniques is 6-bit coding algorithm its used to convert a digitized Audio file into a text file it is show that a good compression ratio between 15-35 \%.},
	
	urldate = {2020-12-09},
	journal = {undefined},
	author = {Mahdi, O. and Mohammed, M. and Mohamed, Ahmed},
	year = {2012},
	file = {Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\X2L8MWCA\\f31713a7d75d6a188d3ada5e89f9243d699e61d4.html:text/html},
}

@article{fredman_pairing_1986,
	title = {The pairing heap: {A} new form of self-adjusting heap},
	volume = {1},
	issn = {1432-0541},
	shorttitle = {The pairing heap},
	url = {https://doi.org/10.1007/BF01840439},
	doi = {10.1007/BF01840439},
	abstract = {Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue) called theFibonacci heap. Although theoretically efficient, Fibonacci heaps are complicated to implement and not as fast in practice as other kinds of heaps. In this paper we describe a new form of heap, called thepairing heap, intended to be competitive with the Fibonacci heap in theory and easy to implement and fast in practice. We provide a partial complexity analysis of pairing heaps. Complete analysis remains an open problem.},
	
	number = {1},
	urldate = {2020-12-09},
	journal = {Algorithmica},
	author = {Fredman, Michael L. and Sedgewick, Robert and Sleator, Daniel D. and Tarjan, Robert E.},
	month = nov,
	year = {1986},
	pages = {111--129},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\T69WAEE6\\Fredman et al. - 1986 - The pairing heap A new form of self-adjusting hea.pdf:application/pdf},
}

@article{williams_algorithm_1964,
	title = {Algorithm 232, {Heapsort}},
	volume = {7},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/512274.512284},
	doi = {10.1145/512274.512284},
	number = {6},
	urldate = {2020-12-09},
	journal = {Communications of the ACM},
	author = {Williams, John William Joseph},
	month = jun,
	year = {1964},
	pages = {347--349},
	file = {Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\7LQ83EQY\\Forsythe - 1964 - Algorithms.pdf:application/pdf},
}

@incollection{larkin_back--basics_2013,
	series = {Proceedings},
	title = {A {Back}-to-{Basics} {Empirical} {Study} of {Priority} {Queues}},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611973198.7},
	abstract = {The theory community has proposed several new heap variants in the recent past which have remained largely untested experimentally. We take the field back to the drawing board, with straightforward implementations of both classic and novel structures using only standard, well-known optimizations. We study the behavior of each structure on a variety of inputs, including artificial workloads, workloads generated by running algorithms on real map data, and workloads from a discrete event simulator used in recent systems networking research. We provide observations about which characteristics are most correlated to performance. For example, we find that the L1 cache miss rate appears to be strongly correlated with wallclock time. We also provide observations about how the input sequence affects the relative performance of the different heap variants. For example, we show (both theoretically and in practice) that certain random insertion-deletion sequences are degenerate and can lead to misleading results. Overall, our findings suggest that while the conventional wisdom holds in some cases, it is sorely mistaken in others.},
	urldate = {2020-12-09},
	booktitle = {2014 {Proceedings} of the {Meeting} on {Algorithm} {Engineering} and {Experiments} ({ALENEX})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Larkin, Daniel H. and Sen, Siddhartha and Tarjan, Robert E.},
	month = dec,
	year = {2013},
	doi = {10.1137/1.9781611973198.7},
	pages = {61--72},
	file = {Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\KHPCWPPW\\1.9781611973198.html:text/html;Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\RPDIQI6L\\Larkin et al. - 2013 - A Back-to-Basics Empirical Study of Priority Queue.pdf:application/pdf},
}

@inproceedings{dinklage_compression_2017,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Compression with the tudocomp {Framework}},
	volume = {75},
	isbn = {978-3-95977-036-1},
	url = {http://drops.dagstuhl.de/opus/volltexte/2017/7601},
	doi = {10.4230/LIPIcs.SEA.2017.13},
	urldate = {2020-12-09},
	booktitle = {16th {International} {Symposium} on {Experimental} {Algorithms} ({SEA} 2017)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Dinklage, Patrick and Fischer, Johannes and Köppl, Dominik and Löbel, Marvin and Sadakane, Kunihiko},
	editor = {Iliopoulos, Costas S. and Pissis, Solon P. and Puglisi, Simon J. and Raman, Rajeev},
	year = {2017},
	note = {ISSN: 1868-8969},
	keywords = {lossless compression, algorithm engineering, application of string algorithms, compression framework, compression library},
	pages = {13:1--13:22},
	file = {Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\CCTT28VS\\7601.html:text/html;Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\VNSHYM92\\Dinklage et al. - 2017 - Compression with the tudocomp Framework.pdf:application/pdf},
}

@article{salson_dynamic_2010,
	series = {Selected papers from the 3rd {Algorithms} and {Complexity} in {Durham} {Workshop} {ACiD} 2007},
	title = {Dynamic extended suffix arrays},
	volume = {8},
	issn = {1570-8667},
	url = {http://www.sciencedirect.com/science/article/pii/S1570866709000343},
	doi = {10.1016/j.jda.2009.02.007},
	abstract = {The suffix tree data structure has been intensively described, studied and used in the eighties and nineties, its linear-time construction counterbalancing his space-consuming requirements. An equivalent data structure, the suffix array, has been described by Manber and Myers in 1990. This space-economical structure has been neglected during more than a decade, its construction being too slow. Since 2003, several linear-time suffix array construction algorithms have been proposed, and this structure has slowly replaced the suffix tree in many string processing problems. All these constructions are building the suffix array from the text, and any edit operation on the text leads to the construction of a brand new suffix array. In this article, we are presenting an algorithm that modifies the suffix array and the Longest Common Prefix (LCP) array when the text is edited (insertion, substitution or deletion of a letter or a factor). This algorithm is based on a recent four-stage algorithm developed for dynamic Burrows–Wheeler Transforms (BWT). For minimizing the space complexity, we are sampling the Suffix Array, a technique used in BWT-based compressed indexes. We furthermore explain how this technique can be adapted for maintaining a sample of the Extended Suffix Array, containing a sample of the Suffix Array, a sample of the Inverse Suffix Array and the whole LCP array. Our practical experiments show that it operates very well in practice, being quicker than the fastest suffix array construction algorithm.},
	
	number = {2},
	urldate = {2021-01-29},
	journal = {Journal of Discrete Algorithms},
	author = {Salson, M. and Lecroq, T. and Léonard, M. and Mouchard, L.},
	month = jun,
	year = {2010},
	keywords = {Suffix array, Algorithm design, Dynamic, Edit operations, FM-index, Self-index data structures},
	pages = {241--257},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\DABA7JWU\\Salson et al. - 2010 - Dynamic extended suffix arrays.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\8F4U4PCH\\S1570866709000343.html:text/html},
}

@article{ganguly_succinct_2020,
	title = {Succinct {Non}-overlapping {Indexing}},
	volume = {82},
	issn = {1432-0541},
	url = {https://doi.org/10.1007/s00453-019-00605-5},
	doi = {10.1007/s00453-019-00605-5},
	abstract = {Text indexing is a fundamental problem in computer science. The objective is to preprocess a text T, so that, given a pattern P, we can find all starting positions (or simply, occurrences) of P in \$\$T\$\$T efficiently. In some cases, additional restrictions are imposed. We consider two variants, namely the non-overlapping indexing problem, and the range non-overlapping indexing problem. Given a text \$\$T\$\$T having n characters, the non-overlapping indexing problem is defined as follows: pre-process \$\$T\$\$T into a data structure, such that for any pattern P, containing {\textbar}P{\textbar} characters, we can report a set containing the maximum number of non-overlapping occurrences of P in \$\$T\$\$T. Cohen and Porat (in: Algorithms and computation, 20th international symposium, ISAAC 2009, Honolulu, Hawaii. Proceedings, 2009) showed that by maintaining a linear space index in which the suffix tree of \$\$T\$\$T is augmented with an O(n) word data structure, a query P can be answered in optimal time \$\$O({\textbar}P{\textbar}+nocc)\$\$O({\textbar}P{\textbar}+nocc), where \$\$nocc\$\$nocc is the number of occurrences reported. We present the following new result. Let \$\${\textbackslash}mathsf \{CSA\}\$\$CSA (not necessarily a compressed suffix array) be an index of \$\$T\$\$T that can compute (i) the suffix range of P in \$\${\textbackslash}mathsf \{search\}(P)\$\$search(P) time, and (ii) a suffix array or an inverse suffix array value in \$\${\textbackslash}mathsf \{t\}\_{\textbackslash}mathsf \{SA\}\$\$tSA time. By using \$\${\textbackslash}mathsf \{CSA\}\$\$CSA alone, we can answer a query P in \$\${\textbackslash}mathsf \{search\}(P)+{\textbackslash}mathsf \{sort\}(nocc)+O(nocc{\textbackslash}cdot {\textbackslash}mathsf \{t\}\_{\textbackslash}mathsf \{SA\})\$\$search(P)+sort(nocc)+O(nocc·tSA) time. The function \$\${\textbackslash}mathsf \{sort\}(k)\$\$sort(k) denotes the time for sorting k numbers in \$\${\textbackslash}\{1,2,{\textbackslash}dots ,n{\textbackslash}\}\$\$\{1,2,⋯,n\}. In the range non-overlapping indexing problem, along with the pattern P, two integers a and b, \$\$b {\textbackslash}ge a\$\$b≥a, are provided as input. The task is to report a set containing the maximum number of non-overlapping occurrences of P that lie within the range [a, b]. For any arbitrarily small positive constant \$\${\textbackslash}epsilon \$\$ϵ, we present an \$\$O(n {\textbackslash}log {\textasciicircum}{\textbackslash}epsilon n)\$\$O(nlogϵn) word index with \$\$O({\textbar}P{\textbar} + nocc\_\{a,b\})\$\$O({\textbar}P{\textbar}+nocca,b) query time, where \$\$nocc\_\{a,b\}\$\$nocca,b is the number of occurrences reported. Our index improves upon the result of Cohen and Porat [6].},
	
	number = {1},
	urldate = {2021-02-01},
	journal = {Algorithmica},
	author = {Ganguly, Arnab and Shah, Rahul and Thankachan, Sharma V.},
	month = jan,
	year = {2020},
	pages = {107--117},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\FBFQ2Y5Z\\Ganguly et al. - 2020 - Succinct Non-overlapping Indexing.pdf:application/pdf},
}

@article{larsson_off-line_2000,
	title = {Off-line dictionary-based compression},
	volume = {88},
	issn = {1558-2256},
	doi = {10.1109/5.892708},
	abstract = {Dictionary-based modeling is a mechanism used in many practical compression schemes. In most implementations of dictionary-based compression the encoder operates on-line, incrementally inferring its dictionary of available phrases from previous parts of the message. An alternative approach is to use the full message to infer a complete dictionary in advance, and include an explicit representation of the dictionary as part of the compressed message. In this investigation, we develop a compression scheme that is a combination of a simple but powerful phrase derivation method and a compact dictionary encoding. The scheme is highly efficient, particularly in decompression, and has characteristics that make it a favorable choice when compressed data is to be searched directly. We describe data structures and algorithms that allow our mechanism to operate in linear time and space.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Larsson, N. J. and Moffat, A.},
	month = nov,
	year = {2000},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {data compression, Data compression, Dictionaries, Encoding, grammars, Computer science, data structures, linear time, \_read, Australia Council, available phrases, compact dictionary encoding, compressed message, Data structures, Decoding, decompression, entropy codes, linear space, off-line dictionary-based compression, phrase derivation method, Resource management, Software engineering},
	pages = {1722--1732},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\2FLJY2HT\\892708.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\UHB99VC4\\Larsson und Moffat - 2000 - Off-line dictionary-based compression.pdf:application/pdf},
}

@inproceedings{abouelhoda_optimal_2002,
	title = {Optimal {Exact} {Strring} {Matching} {Based} on {Suffix} {Arrays}},
	doi = {10.1007/3-540-45735-6_4},
	abstract = {Using the suffix tree of a string S, decision queries of the type "Is P a substring of S?" can be answered in O({\textbar}P{\textbar}) time and enumeration queries of the type "Where are all z occurrences of P in S?" can be answered in O({\textbar}P{\textbar}+z) time, totally independent of the size of S. However, in large scale applications as genome analysis, the space requirements of the suffix tree are a severe drawback. The suffix array is a more space economical index structure. Using it and an additional table, Manber and Myers (1993) showed that decision queries and enumeration queries can be answered in O({\textbar}P{\textbar}+log {\textbar}S{\textbar}) and O({\textbar}P{\textbar}+log {\textbar}S{\textbar}+z) time, respectively, but no optimal time algorithms are known. In this paper, we show how to achieve the optimal O({\textbar}P{\textbar}) and O({\textbar}P{\textbar} + z) time bounds for the suffix array. Our approach is not confined to exact pattern matching. In fact, it can be used to efficiently solve all problems that are usually solved by a top-down traversal of the suffix tree. Experiments show that our method is not only of theoretical interest but also of practical relevance.},
	booktitle = {{SPIRE}},
	author = {Abouelhoda, M. and Ohlebusch, E. and Kurtz, S.},
	year = {2002},
	keywords = {\_reading},
	file = {Abouelhoda et al. - 2002 - Optimal Exact Strring Matching Based on Suffix Arr.pdf:C\:\\Users\\etiet\\Zotero\\storage\\BPBLU75S\\Abouelhoda et al. - 2002 - Optimal Exact Strring Matching Based on Suffix Arr.pdf:application/pdf},
}

@inproceedings{rahman_optimised_2001,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimised {Predecessor} {Data} {Structures} for {Internal} {Memory}},
	isbn = {978-3-540-44688-0},
	doi = {10.1007/3-540-44688-5_6},
	abstract = {We demonstrate the importance of reducing misses in the translation-lookaside buffer (TLB) for obtaining good performance on modern computer architectures. We focus on data structures for the dynamic predecessor problem: to maintain a set S of keys from a totally ordered universe under insertions, deletions and predecessor queries. We give two general techniques for simultaneously reducing cache and TLB misses: simulating 3-level hierarchical memory algorithms and cache-oblivious algorithms. We give preliminary experimental results which demonstrate that data structures based on these ideas outperform data structures which are based on minimising cache misses alone, namely B-tree variants.},
	
	booktitle = {Algorithm {Engineering}},
	publisher = {Springer},
	author = {Rahman, Naila and Cole, Richard and Raman, Rajeev},
	editor = {Brodal, Gerth Stølting and Frigioni, Daniele and Marchetti-Spaccamela, Alberto},
	year = {2001},
	keywords = {Cache Block, External Node, Main Memory, Search Tree, Secondary Memory},
	pages = {67--78},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\98ZNASHR\\Rahman et al. - 2001 - Optimised Predecessor Data Structures for Internal.pdf:application/pdf},
}

@inproceedings{dinklage_engineering_2021,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Engineering {Predecessor} {Data} {Structure} for {Dynamic} {Integer} {Sets}},
	abstract = {We present highly optimized data structures for the dynamic predecessor problem, where the task is
to maintain a set S of w-bit numbers under insertions, deletions, and predecessor queries (return the
largest element in S no larger than a given key). The problem of finding predecessors can be viewed
as a generalized form of the membership problem, or as a simple version of the nearest neighbour
problem. It lies at the core of various real-world problems such as internet routing.
In this work, we engineer (1) a simple implementation of the idea of universe reduction, similar
to van-Emde-Boas trees (2) variants of y-fast tries [Willard, IPL’83], and (3) B-trees with different
strategies for organizing the keys contained in the nodes, including an implementation of dynamic
fusion nodes [Pˇatraşcu and Thorup, FOCS’14]. We implement our data structures for w = 32, 40, 64,
which covers most typical scenarios.
Our data structures finish workloads faster than previous approaches while being significantly
more space-efficient, e.g., they clearly outperform standard implementations of the STL by finishing
up to four times as fast using less than a third of the memory. Our tests also provide more general
insights on data structure design, such as how small sets should be stored and handled and if and
when new CPU instructions such as advanced vector extensions pay off.},
	language = {German},
	booktitle = {19th {International} {Symposium} on {Experimental} {Algorithms} ({SEA} 2021)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Dinklage, Patrick and Fischer, Johannes and Herlez, Alexander},
	year = {2021},
}

@inproceedings{guibas_dichromatic_1978,
	title = {A dichromatic framework for balanced trees},
	doi = {10.1109/SFCS.1978.3},
	abstract = {In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.},
	booktitle = {19th {Annual} {Symposium} on {Foundations} of {Computer} {Science} (sfcs 1978)},
	author = {Guibas, Leo J. and Sedgewick, Robert},
	month = oct,
	year = {1978},
	note = {ISSN: 0272-5428},
	keywords = {Computer science, Algorithm design and analysis, Particle measurements, Performance analysis, Petroleum},
	pages = {8--21},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\MS8WV2KF\\4567957.html:text/html},
}

@inproceedings{guibas_dichromatic_1978-1,
	title = {A dichromatic framework for balanced trees},
	doi = {10.1109/SFCS.1978.3},
	abstract = {In this paper we present a uniform framework for the implementation and study of balanced tree algorithms. We show how to imbed in this framework the best known balanced tree techniques and then use the framework to develop new algorithms which perform the update and rebalancing in one pass, on the way down towards a leaf. We conclude with a study of performance issues and concurrent updating.},
	booktitle = {19th {Annual} {Symposium} on {Foundations} of {Computer} {Science} (sfcs 1978)},
	author = {Guibas, Leo J. and Sedgewick, Robert},
	month = oct,
	year = {1978},
	note = {ISSN: 0272-5428},
	keywords = {Computer science, Algorithm design and analysis, Particle measurements, Performance analysis, Petroleum},
	pages = {8--21},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\6N2DZQME\\Guibas und Sedgewick - 1978 - A dichromatic framework for balanced trees.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\etiet\\Zotero\\storage\\F2XPCLS4\\4567957.html:text/html},
}

@article{bayer_symmetric_1972,
	title = {Symmetric binary {B}-{Trees}: {Data} structure and maintenance algorithms},
	volume = {1},
	issn = {1432-0525},
	shorttitle = {Symmetric binary {B}-{Trees}},
	url = {https://doi.org/10.1007/BF00289509},
	doi = {10.1007/BF00289509},
	abstract = {A class of binary trees is described for maintaining ordered sets of data. Random insertions, deletions, and retrievals of keys can be done in time proportional to log N where N is the cardinality of the data-set. Symmetric B-Trees are a modification of B-trees described previously by Bayer and McCreight. This class of trees properly contains the balanced trees.},
	
	number = {4},
	urldate = {2021-04-23},
	journal = {Acta Informatica},
	author = {Bayer, Rudolf},
	month = dec,
	year = {1972},
	pages = {290--306},
	file = {Springer Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\X2YPJCU9\\Bayer - 1972 - Symmetric binary B-Trees Data structure and maint.pdf:application/pdf},
}

@article{larsson_faster_2007,
	series = {The {Burrows}-{Wheeler} {Transform}},
	title = {Faster suffix sorting},
	volume = {387},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397507005257},
	doi = {10.1016/j.tcs.2007.07.017},
	abstract = {We propose a fast and memory-efficient algorithm for lexicographically sorting the suffixes of a string, a problem that has important applications in data compression as well as string matching. Our algorithm eliminates much of the overhead of previous specialized approaches while maintaining their robustness for degenerate inputs. For input size n, our algorithm operates in only two integer arrays of size n, and has worst-case time complexity O(nlogn). We demonstrate experimentally that our algorithm has stable performance compared with other approaches.},
	
	number = {3},
	urldate = {2021-05-01},
	journal = {Theoretical Computer Science},
	author = {Larsson, N. Jesper and Sadakane, Kunihiko},
	month = nov,
	year = {2007},
	keywords = {Burrows–Wheeler transform, Suffix arrays},
	pages = {258--272},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\etiet\\Zotero\\storage\\PUMDDUFP\\Larsson und Sadakane - 2007 - Faster suffix sorting.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\etiet\\Zotero\\storage\\UYS54X5D\\S0304397507005257.html:text/html},
}
